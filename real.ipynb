{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import dependencies\n",
    "import IPython\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import dlib\n",
    "import numpy as np\n",
    "import imutils\n",
    "from scipy.spatial import distance as dist\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from scipy.spatial.distance import euclidean as dist\n",
    "from IPython.display import Javascript, display\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "from imutils.face_utils import FaceAligner\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "import base64\n",
    "from PIL import Image\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play an audio beep. Any audio URL will do.\n",
    "alert='new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()'\n",
    "bienvenue='new Audio(\"https://upload.wikimedia.org/wikipedia/commons/1/1a/Bienvenue-chez-votre-applicati1651880044.ogg\").play()'\n",
    "pause='new Audio(\"https://upload.wikimedia.org/wikipedia/commons/7/7e/Penser-a-prendre-une-petite-pa1651880436.ogg\").play()'\n",
    "reveil='new Audio(\"https://upload.wikimedia.org/wikipedia/commons/2/23/R%C3%A9veillez-vousr%C3%A9veillez-vous-1651880280_%281%29.ogg\").play()'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model=load_model(r\"C:\\Users\\User\\Documents\\Desktop\\Projet\\somnolance.keras\")\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "face_cascade = cv2.CascadeClassifier(r'C:\\Users\\User\\Documents\\Desktop\\Projet\\Hardcascad_file\\haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(r'C:\\Users\\User\\Documents\\Desktop\\Projet\\Hardcascad_file\\haarcascade_eye.xml')\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(r'C:\\Users\\User\\Documents\\Desktop\\Projet\\shape_predictor_68_face_landmarks.dat')\n",
    "fa = FaceAligner(predictor, desiredFaceWidth=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_lip(lips):\n",
    "    # Vérifier que les points clés sont valides\n",
    "    if len(lips) < 7:\n",
    "        raise ValueError(\"Le tableau 'lips' doit contenir au moins 7 points.\")\n",
    "\n",
    "    # Calculer les distances\n",
    "    dist1 = dist(lips[2], lips[6])  # Hauteur de l'ouverture de la bouche\n",
    "    dist2 = dist(lips[0], lips[4])  # Largeur de la bouche\n",
    "\n",
    "    # Éviter une division par zéro\n",
    "    if dist2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculer le ratio LAR\n",
    "    LAR = float(dist1 / dist2)\n",
    "\n",
    "    return LAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def js_to_image(js_reply):\n",
    "    \"\"\"\n",
    "    Convertit une image JavaScript (base64) en une image OpenCV au format BGR.\n",
    "\n",
    "    Args:\n",
    "        js_reply (str): Chaîne JavaScript contenant l'image au format base64.\n",
    "\n",
    "    Returns:\n",
    "        img (numpy.ndarray): Image OpenCV au format BGR.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si le format de `js_reply` est invalide.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extraire la partie base64 de la chaîne JavaScript\n",
    "        if ',' not in js_reply:\n",
    "            raise ValueError(\"Format de js_reply invalide. Attendu: 'data:image/...;base64,...'\")\n",
    "        \n",
    "        # Décoder l'image en base64\n",
    "        image_bytes = base64.b64decode(js_reply.split(',')[1])\n",
    "        \n",
    "        # Convertir les octets en tableau numpy\n",
    "        jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "        \n",
    "        # Décoder le tableau numpy en image OpenCV au format BGR\n",
    "        img = cv2.imdecode(jpg_as_np, flags=cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if img is None:\n",
    "            raise ValueError(\"Échec de la décodage de l'image.\")\n",
    "        \n",
    "        return img\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la conversion de l'image : {e}\")\n",
    "\n",
    "\n",
    "def bbox_to_bytes(bbox_array):\n",
    "    \"\"\"\n",
    "    Convertit un tableau numpy (boîte englobante) en une chaîne base64 pour l'affichage.\n",
    "\n",
    "    Args:\n",
    "        bbox_array (numpy.ndarray): Tableau numpy (pixels) contenant la boîte englobante au format RGBA.\n",
    "\n",
    "    Returns:\n",
    "        str: Chaîne base64 de l'image PNG encodée.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si `bbox_array` n'est pas un tableau numpy valide.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Vérifier que l'entrée est un tableau numpy\n",
    "        if not isinstance(bbox_array, np.ndarray):\n",
    "            raise ValueError(\"bbox_array doit être un tableau numpy.\")\n",
    "        \n",
    "        # Convertir le tableau en image PIL\n",
    "        bbox_PIL = Image.fromarray(bbox_array, 'RGBA')\n",
    "        \n",
    "        # Utiliser un tampon mémoire pour enregistrer l'image\n",
    "        with io.BytesIO() as iobuf:\n",
    "            # Enregistrer l'image au format PNG dans le tampon\n",
    "            bbox_PIL.save(iobuf, format='PNG')\n",
    "            # Encoder l'image en base64\n",
    "            bbox_bytes = base64.b64encode(iobuf.getvalue()).decode('utf-8')\n",
    "        \n",
    "        # Formater la chaîne de retour\n",
    "        return f'data:image/png;base64,{bbox_bytes}'\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la conversion de la boîte englobante : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_stream():\n",
    "    js = Javascript(\n",
    "'''\n",
    "var video;\n",
    "var div = null;\n",
    "var stream;\n",
    "var captureCanvas;\n",
    "var imgElement;\n",
    "var labelElement;\n",
    "\n",
    "var pendingResolve = null;\n",
    "var shutdown = false;\n",
    "\n",
    "/**\n",
    " * Nettoie et supprime les éléments DOM créés pour le flux vidéo.\n",
    " */\n",
    "function removeDom() {\n",
    "    if (stream) {\n",
    "        stream.getVideoTracks()[0].stop(); // Arrête le flux vidéo\n",
    "    }\n",
    "    if (video) video.remove();\n",
    "    if (div) div.remove();\n",
    "    if (imgElement) imgElement.remove();\n",
    "    if (captureCanvas) captureCanvas.remove();\n",
    "    if (labelElement) labelElement.remove();\n",
    "\n",
    "    video = null;\n",
    "    div = null;\n",
    "    stream = null;\n",
    "    imgElement = null;\n",
    "    captureCanvas = null;\n",
    "    labelElement = null;\n",
    "}\n",
    "\n",
    "/**\n",
    " * Fonction appelée à chaque frame d'animation pour capturer une image.\n",
    " */\n",
    "function onAnimationFrame() {\n",
    "    if (shutdown) return;\n",
    "\n",
    "    window.requestAnimationFrame(onAnimationFrame);\n",
    "\n",
    "    if (pendingResolve) {\n",
    "        let result = \"\";\n",
    "        if (!shutdown && captureCanvas && video) {\n",
    "            const context = captureCanvas.getContext('2d');\n",
    "            context.drawImage(video, 0, 0, 640, 480); // Capture une image de la vidéo\n",
    "            result = captureCanvas.toDataURL('image/jpeg', 0.8); // Convertit en base64\n",
    "        }\n",
    "        const resolve = pendingResolve;\n",
    "        pendingResolve = null;\n",
    "        resolve(result); // Renvoie l'image capturée\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * Crée les éléments DOM nécessaires pour afficher le flux vidéo.\n",
    " */\n",
    "async function createDom() {\n",
    "    if (div !== null) return stream;\n",
    "\n",
    "    // Crée un conteneur pour la vidéo\n",
    "    div = document.createElement('div');\n",
    "    div.style.border = '2px solid black';\n",
    "    div.style.padding = '3px';\n",
    "    div.style.width = '100%';\n",
    "    div.style.maxWidth = '600px';\n",
    "    document.body.appendChild(div);\n",
    "\n",
    "    // Crée un élément pour afficher le statut\n",
    "    const modelOut = document.createElement('div');\n",
    "    modelOut.innerHTML = \"<span>Status:</span>\";\n",
    "    labelElement = document.createElement('span');\n",
    "    labelElement.innerText = 'No data';\n",
    "    labelElement.style.fontWeight = 'bold';\n",
    "    modelOut.appendChild(labelElement);\n",
    "    div.appendChild(modelOut);\n",
    "\n",
    "    // Crée l'élément vidéo\n",
    "    video = document.createElement('video');\n",
    "    video.style.display = 'block';\n",
    "    video.width = div.clientWidth - 6;\n",
    "    video.setAttribute('playsinline', '');\n",
    "    video.onclick = () => { shutdown = true; }; // Permet de cliquer pour arrêter\n",
    "\n",
    "    // Demande l'accès à la webcam\n",
    "    try {\n",
    "        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: \"environment\" } });\n",
    "        div.appendChild(video);\n",
    "\n",
    "        // Crée un élément pour afficher les images superposées\n",
    "        imgElement = document.createElement('img');\n",
    "        imgElement.style.position = 'absolute';\n",
    "        imgElement.style.zIndex = 1;\n",
    "        imgElement.onclick = () => { shutdown = true; };\n",
    "        div.appendChild(imgElement);\n",
    "\n",
    "        // Ajoute des instructions pour l'utilisateur\n",
    "        const instruction = document.createElement('div');\n",
    "        instruction.innerHTML = \n",
    "            '<span style=\"color: red; font-weight: bold;\">' +\n",
    "            'When finished, click here or on the video to stop this demo</span>';\n",
    "        div.appendChild(instruction);\n",
    "        instruction.onclick = () => { shutdown = true; };\n",
    "\n",
    "        video.srcObject = stream;\n",
    "        await video.play();\n",
    "\n",
    "        // Crée un canvas pour capturer les images\n",
    "        captureCanvas = document.createElement('canvas');\n",
    "        captureCanvas.width = 640; // Largeur fixe\n",
    "        captureCanvas.height = 480; // Hauteur fixe\n",
    "        window.requestAnimationFrame(onAnimationFrame);\n",
    "\n",
    "        return stream;\n",
    "    } catch (error) {\n",
    "        console.error(\"Erreur lors de l'accès à la webcam :\", error);\n",
    "        removeDom();\n",
    "        throw error;\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * Capture une image du flux vidéo et renvoie les données.\n",
    " * @param {string} label - Le texte à afficher comme statut.\n",
    " * @param {string} imgData - Les données de l'image à superposer.\n",
    " * @returns {Promise<Object>} - Les données de l'image capturée et les timings.\n",
    " */\n",
    "async function stream_frame(label, imgData) {\n",
    "    if (shutdown) {\n",
    "        removeDom();\n",
    "        shutdown = false;\n",
    "        return '';\n",
    "    }\n",
    "\n",
    "    const preCreate = Date.now();\n",
    "    await createDom();\n",
    "\n",
    "    const preShow = Date.now();\n",
    "    if (label) {\n",
    "        labelElement.innerHTML = label;\n",
    "    }\n",
    "\n",
    "    if (imgData) {\n",
    "        const videoRect = video.getClientRects()[0];\n",
    "        imgElement.style.top = `${videoRect.top}px`;\n",
    "        imgElement.style.left = `${videoRect.left}px`;\n",
    "        imgElement.style.width = `${videoRect.width}px`;\n",
    "        imgElement.style.height = `${videoRect.height}px`;\n",
    "        imgElement.src = imgData;\n",
    "    }\n",
    "\n",
    "    const preCapture = Date.now();\n",
    "    const img = await new Promise((resolve) => {\n",
    "        pendingResolve = resolve;\n",
    "    });\n",
    "    shutdown = false;\n",
    "\n",
    "    return {\n",
    "        create: preShow - preCreate,\n",
    "        show: preCapture - preShow,\n",
    "        capture: Date.now() - preCapture,\n",
    "        img: img\n",
    "    };\n",
    "}  \n",
    "\n",
    "''') \n",
    "    display(js)\n",
    "  \n",
    "def video_frame(label, bbox):\n",
    "    data = eval_js(f'stream_frame(\"{label}\", \"{bbox}\")')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_frame(label, bbox):\n",
    "    \"\"\"\n",
    "    Capture une image à partir du flux vidéo de la webcam en utilisant JavaScript.\n",
    "\n",
    "    Args:\n",
    "        label (str): Le texte à afficher comme statut dans l'interface utilisateur.\n",
    "        bbox (str): Les données de l'image à superposer (par exemple, une boîte englobante).\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant les données de l'image capturée et les timings.\n",
    "              Format : {'create': temps, 'show': temps, 'capture': temps, 'img': base64}.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si `label` ou `bbox` ne sont pas des chaînes de caractères valides.\n",
    "        RuntimeError: Si une erreur survient lors de l'exécution du code JavaScript.\n",
    "    \"\"\"\n",
    "    # Validation des entrées\n",
    "    if not isinstance(label, str) or not isinstance(bbox, str):\n",
    "        raise ValueError(\"Les arguments 'label' et 'bbox' doivent être des chaînes de caractères.\")\n",
    "\n",
    "    try:\n",
    "        # Exécuter le code JavaScript pour capturer une image\n",
    "        data = eval_js(f'stream_frame(\"{label}\", \"{bbox}\")')\n",
    "        \n",
    "        # Vérifier que les données retournées sont valides\n",
    "        if not data or not isinstance(data, dict):\n",
    "            raise RuntimeError(\"Aucune donnée valide retournée par le code JavaScript.\")\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        # Capturer et relancer les erreurs avec un message clair\n",
    "        raise RuntimeError(f\"Erreur lors de la capture de l'image : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webview\n",
    "\n",
    "def video_frame(label, bbox):\n",
    "    \"\"\"\n",
    "    Capture une image à partir du flux vidéo de la webcam en utilisant JavaScript.\n",
    "\n",
    "    Args:\n",
    "        label (str): Le texte à afficher comme statut dans l'interface utilisateur.\n",
    "        bbox (str): Les données de l'image à superposer (par exemple, une boîte englobante).\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant les données de l'image capturée et les timings.\n",
    "              Format : {'create': temps, 'show': temps, 'capture': temps, 'img': base64}.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si `label` ou `bbox` ne sont pas des chaînes de caractères valides.\n",
    "        RuntimeError: Si une erreur survient lors de l'exécution du code JavaScript.\n",
    "    \"\"\"\n",
    "    # Validation des entrées\n",
    "    if not isinstance(label, str) or not isinstance(bbox, str):\n",
    "        raise ValueError(\"Les arguments 'label' et 'bbox' doivent être des chaînes de caractères.\")\n",
    "\n",
    "    try:\n",
    "        # Créer une fenêtre Web pour exécuter le JavaScript\n",
    "        window = webview.create_window(\"Webcam Stream\", html=\"<h1>Loading...</h1>\")\n",
    "        webview.start()\n",
    "\n",
    "        # Injecter le code JavaScript\n",
    "        js_code = f'''\n",
    "        stream_frame(\"{label}\", \"{bbox}\")\n",
    "            .then(data => {{\n",
    "                const output = JSON.stringify(data);\n",
    "                console.log(output);  // Afficher les données dans la console\n",
    "                window.pywebview.api.returnData(output);\n",
    "            }});\n",
    "        '''\n",
    "        window.evaluate_js(js_code)\n",
    "\n",
    "        # Attendre que le résultat soit disponible\n",
    "        if 'video_frame_result' in globals():\n",
    "            data = globals()['video_frame_result']\n",
    "            return data\n",
    "        else:\n",
    "            raise RuntimeError(\"Aucune donnée retournée par le code JavaScript.\")\n",
    "    except Exception as e:\n",
    "        # Capturer et relancer les erreurs avec un message clair\n",
    "        raise RuntimeError(f\"Erreur lors de la capture de l'image : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[258], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Ignorer cette image et passer à la suivante\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Détecter les visages\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m rects \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rect \u001b[38;5;129;01min\u001b[39;00m rects:\n\u001b[0;32m     80\u001b[0m     (x, y, w, h) \u001b[38;5;241m=\u001b[39m face_utils\u001b[38;5;241m.\u001b[39mrect_to_bb(rect)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "\n",
    "# Initialisation des détecteurs et prédicteurs\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(r\"shape_predictor_68_face_landmarks.dat\")\n",
    "fa = face_utils.FaceAligner(predictor, desiredFaceWidth=256)\n",
    "\n",
    "# Seuils pour la détection de bâillement et de somnolence\n",
    "LIP_AR_THRESH = 0.4  # Seuil pour le ratio d'ouverture de la bouche\n",
    "EYE_AR_THRESH = 0.25  # Seuil pour le ratio d'ouverture des yeux\n",
    "LIP_PER_FRAME = 1  # Nombre de frames consécutifs pour déclencher une alerte\n",
    "\n",
    "# Fonction pour calculer le ratio d'ouverture de la bouche\n",
    "def calculate_lip(lip_points):\n",
    "    dist1 = dist.euclidean(lip_points[2], lip_points[6])\n",
    "    dist2 = dist.euclidean(lip_points[0], lip_points[4])\n",
    "    return dist1 / dist2\n",
    "\n",
    "# Démarrer le flux vidéo\n",
    "def video_stream():\n",
    "    global video_capture\n",
    "    video_capture = cv2.VideoCapture(0)\n",
    "    if not video_capture.isOpened():\n",
    "        raise ValueError(\"La caméra ne peut pas être ouverte.\")\n",
    "\n",
    "def video_frame():\n",
    "    # Lire une image du flux vidéo\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        return False\n",
    "    return frame\n",
    "\n",
    "# Définir une fonction pour afficher l'alerte\n",
    "def alert():\n",
    "    print(\"ALERTE: Somnolence détectée!\")\n",
    "\n",
    "def reveil():\n",
    "    print(\"Réveil: Le conducteur est alerte.\")\n",
    "\n",
    "def pause():\n",
    "    print(\"Pause: Le conducteur doit se reposer.\")\n",
    "\n",
    "def bbox_to_bytes(image):\n",
    "    # Convertir l'image en PNG et l'encoder en base64\n",
    "    _, buffer = cv2.imencode('.png', image)\n",
    "    return buffer.tobytes()\n",
    "\n",
    "# Initialisation des variables\n",
    "counter = 0\n",
    "bbox = ''\n",
    "\n",
    "# Démarrer le flux vidéo\n",
    "video_stream()\n",
    "\n",
    "# Boucle principale de détection\n",
    "while True:\n",
    "    # Capturer une image du flux vidéo\n",
    "    img = video_frame()\n",
    "    if img is False:\n",
    "        break\n",
    "\n",
    "    # Créer un overlay transparent pour la boîte englobante\n",
    "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
    "\n",
    "    # Convertir l'image en niveaux de gris\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Vérification si l'image est bien en niveaux de gris (8 bits)\n",
    "    if len(gray.shape) != 2:\n",
    "        print(\"Erreur: l'image n'est pas en niveaux de gris\")\n",
    "        continue  # Ignorer cette image et passer à la suivante\n",
    "\n",
    "    # Détecter les visages\n",
    "    rects = detector(gray, 2)\n",
    "    for rect in rects:\n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face_aligned = fa.align(img, gray, rect)\n",
    "        face_aligned_gray = cv2.cvtColor(face_aligned, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Détecter les yeux\n",
    "        eyes = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\").detectMultiScale(face_aligned_gray, 1.1, 4)\n",
    "        predictions = []\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            eye = face_aligned[ey:ey+eh, ex:ex+ew]\n",
    "            eye = cv2.resize(eye, (32, 32))\n",
    "            eye = np.expand_dims(eye, axis=0)\n",
    "            \n",
    "            # Ici, vous devez charger ou définir un modèle pour la prédiction des yeux\n",
    "            # Exemple : model = load_model('eye_model.h5') avant d'utiliser model.predict()\n",
    "            # ypred = model.predict(eye)\n",
    "            ypred = np.array([0])  # Remplacez par votre prédiction du modèle\n",
    "            ypred = np.argmax(ypred[0], axis=0)\n",
    "            predictions.append(ypred)\n",
    "\n",
    "        # Vérifier si les yeux sont fermés\n",
    "        if all(i == 0 for i in predictions):\n",
    "            cv2.rectangle(bbox_array, (x, y), (x+w, y+h), (255, 0, 0), 8)\n",
    "            cv2.putText(bbox_array, 'Sleeping', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 3)\n",
    "            alert()\n",
    "            reveil()\n",
    "        else:\n",
    "            cv2.rectangle(bbox_array, (x, y), (x+w, y+h), (0, 255, 0), 8)\n",
    "            cv2.putText(bbox_array, 'Not Sleeping', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "\n",
    "        # Détecter les bâillements\n",
    "        lip_points = [60, 61, 62, 63, 64, 65, 66, 67]\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        lip_shape = shape[lip_points]\n",
    "        LAR = calculate_lip(lip_shape)\n",
    "\n",
    "        # Dessiner les contours des lèvres\n",
    "        lip_hull = cv2.convexHull(lip_shape)\n",
    "        cv2.drawContours(bbox_array, [lip_hull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        # Vérifier si la bouche est ouverte\n",
    "        if LAR > LIP_AR_THRESH:\n",
    "            counter += 1\n",
    "            if counter > LIP_PER_FRAME:\n",
    "                cv2.putText(bbox_array, \"YAWNING ALERT!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "                pause()\n",
    "        else:\n",
    "            counter = 0\n",
    "\n",
    "        # Afficher le ratio d'ouverture de la bouche\n",
    "        cv2.putText(bbox_array, \"LAR: {:.2f}\".format(LAR), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Convertir l'overlay en bytes\n",
    "    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255\n",
    "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
    "\n",
    "    # Mettre à jour la boîte englobante pour la prochaine frame\n",
    "    bbox = bbox_bytes\n",
    "\n",
    "    # Afficher l'image\n",
    "    cv2.imshow(\"Frame\", img)\n",
    "\n",
    "    # Quitter si la touche 'q' est pressée\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Libérer la capture et fermer les fenêtres\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRO_IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
